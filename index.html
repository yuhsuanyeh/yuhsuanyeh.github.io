<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yu-Hsuan Yeh</title>
  
  <meta name="author" content="Yu-Hsuan Yeh">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="shortcut icon" href="favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yu-Hsuan Yeh</name>
              </p>
              <p>I got my second M.S. degree at <a target="_blank" href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/">CMU MSCV</a>, 
		 and a first M.Sc. from National Yang Ming Chiao Tung University(NYCU) advised by <a target="_blank" href="https://walonchiu.github.io/">Prof. Wei-Chen Chiu</a>, 
		 <a target="_blank" href="https://aliensunmin.github.io/">Prof. Min Sun</a>,and <a target="_blank" href="https://sites.google.com/site/yihsuantsai/">Dr. Yi-Hsuan Tsai</a>.
              </p>
              <p>Currently, I work as an Applied Scientist at Adobe, where I contribute to the <a target="_blank" href="https://www.adobe.com/products/firefly/landpa.html?sdid=YFRVGB2T&mv=search&mv2=paidsearch&ef_id=c5c0c2b7ae6511314a57aad66c43e42e:G:s&s_kwcid=AL!3085!10!79027608896838!!!!79027989175720!!506073772!1264439369271988">Firefly</a> GenAI team, focusing on generative AI product development. My background spans both academic and industrial research, with publications in top-tier conferences and hands-on experience bringing advanced machine learning systems into production.
	      <p style="color: #d0176d" style="text-align: justify;"><strong>I am actively seeking opportunities as an Applied Scientist or Machine Learning Engineer</strong>, particularly in areas involving deep learning, generative models, or large-scale training pipelines.</p> 
	      <p> Please feel free to reach out if you're hiring or open to collaboration.
	      </p>
              <p style="text-align:center">
                Email: alice12595@gmail.com <br/>
                <a target="_blank" href="data/yuhsuanyeh_resume_2025.pdf">Resume</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com.tw/citations?user=BB2AtBgAAAAJ&hl=zh-TW">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/yuhsuanyeh/">Github</a> &nbsp/&nbsp
		<a target="_blank" href="https://www.linkedin.com/in/yeh-yu-hsuan-7b6ba0205/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a target="_blank" href="images/Yu_Hsuan_head.png"><img style="width:80%;max-width:80%" alt="profile photo" src="images/Yu_Hsuan_head.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interest mainly focuses on deep learning and computer vision.  
              I am currently working on projects related to scene understanding, such as depth and layout estimation. 
              My goal is to build enriched vision applications and reliable AI systems that can perceive various environments and produce robust estimations for humans. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="photoscene_stop()" onmouseover="photoscene_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/paper_teaser/Bifuse2_teaser.png' width="110%">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                                <a target="_blank" href="https://arxiv.org/abs/2209.02952">
                    <papertitle>BiFuse++: Self-supervised and Efficient Bi-projection Fusion for 360 Depth Estimation</papertitle>
                  </a>
                  <br>  
                  <a target="_blank" href="https://fuenwang.phd/">Fu-En Wang</a>,
                  <strong>Yu-Hsuan Yeh</strong>,
                  <a target="_blank" href="https://aliensunmin.github.io/">Min Sun</a>, 
                  <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, 
                  <a target="_blank" href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>
              <br>
              <em>TPAMI</em>, 2022 &nbsp <font color=#d0176d></font>
                  <br>
                  <a target="_blank" href="https://arxiv.org/abs/2209.02952">paper</a> / 
                  <a target="_blank" href="https://github.com/fuenwang/BiFusev2">code</a>
                  <p></p>
                  <p>We propose a new fusion module and Contrast-Aware Photometric Loss to improve the performance of BiFuse and increase the stability of self-training on real-world videos.</p>
                </td>
              </tr>

            <tr onmouseout="photoscene_stop()" onmouseover="photoscene_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='LED2_image'><img src='images/paper_teaser/final_yeh_light.gif' width="110%"></div>
                <img src='images/paper_teaser/LED2_teaser.png' width="110%">
              </div>
              <script type="text/javascript">
                function photoscene_start() {
                  document.getElementById('LED2_image').style.opacity = "1";
                }

                function photoscene_stop() {
                  document.getElementById('LED2_image').style.opacity = "0";
                }
                photoscene_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a target="_blank" href="https://fuenwang.phd/project/led2net/">
                <papertitle>LED2-Net: Monocular 360˚ Layout Estimation via Differentiable Depth Rendering</papertitle>
              </a>
              <br>
              <strong>*Yu-Hsuan Yeh</strong>,  
              <a target="_blank" href="https://fuenwang.phd/">*Fu-En Wang</a>, 
              <a target="_blank" href="https://aliensunmin.github.io/">Min Sun</a>, 
              <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, 
              <a target="_blank" href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>
	      <br> 
	      <em>CVPR</em>, 2021 &nbsp <font color=#d0176d><strong>(Oral Presentation)</strong></font>  
              <br>
              <a target="_blank" href="https://fuenwang.phd/project/led2net/">project page</a> / 
              <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_LED2-Net_Monocular_360deg_Layout_Estimation_via_Differentiable_Depth_Rendering_CVPR_2021_paper.pdf">cvpr paper</a> / 
              <a target="_blank" href="https://github.com/fuenwang/LED2-Net">code</a>
              <p></p>
              <p>We propose a differentiable layout-to-depth procedure to convert the 360 layout representation into the 360 horizon-depth map, thus enabling the training objective for our layout estimation network to take advantage of 3D geometric information.</p>
            </td>
          </tr>
          
          <tr onmouseout="transparent_stop()" onmouseover="transparent_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/paper_teaser/1690-teaser.gif' width="110%">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="https://fuenwang.phd/project/bifuse/?fbclid=IwAR0qKLtkrkGgtGiu2zdm7YpW-3DkBPzdYvonWYUqyv1yKFA1nLYz5Q5qqEM">
                <papertitle>BiFuse: Monocular 360◦ Depth Estimation via Bi-Projection Fusion</papertitle>
              </a>
              <br> 
              <strong>*Yu-Hsuan Yeh</strong>,
              <a target="_blank" href="https://fuenwang.phd/">*Fu-En Wang</a>, 
              <a target="_blank" href="https://aliensunmin.github.io/">Min Sun</a>, 
              <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, 
              <a target="_blank" href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>
	      <br>
              <em>CVPR</em>, 2020
              <br>
                <a target="_blank" href="https://fuenwang.phd/project/bifuse/?fbclid=IwAR0qKLtkrkGgtGiu2zdm7YpW-3DkBPzdYvonWYUqyv1yKFA1nLYz5Q5qqEM">project page</a> / 
		        <a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_BiFuse_Monocular_360_Depth_Estimation_via_Bi-Projection_Fusion_CVPR_2020_paper.pdf">cvpr paper</a> /
                <a target="_blank" href="https://github.com/Yeh-yu-hsuan/BiFuse">code</a>
              <p></p>
              <p>We propose a two-branch neural network leveraging two common projections – equirectangular and cubemap projections – as inputs to predict the depth map of a monocular 360 image.</p>
            </td>
          </tr>

          <tr onmouseout="transparent_stop()" onmouseover="transparent_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/paper_teaser/LayoutMP3D.png' width="110%">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="https://arxiv.org/pdf/2003.13516.pdf">
                <papertitle>LayoutMP3D: Layout Annotation of Matterport3D</papertitle>
              </a>
              <br> 
              <strong>*Yu-Hsuan Yeh</strong>,
              <a target="_blank" href="https://fuenwang.phd/">*Fu-En Wang</a>, 
              <a target="_blank" href="https://aliensunmin.github.io/">Min Sun</a>, 
              <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, 
              <a target="_blank" href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>
	      <br>
              <em>Arxiv</em>, 2020
              <br> 
		        <a target="_blank" href="https://arxiv.org/pdf/2003.13516.pdf">paper</a> /
                <a target="_blank" href="https://github.com/fuenwang/LayoutMP3D">code</a>
              <p></p>
              <p>We release the first real-world dataset containing paired depth and layout annotations.</p>
            </td>
          </tr>

        </tbody></table>
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This page is adapted from this <a target="_blank" href="https://github.com/jonbarron/jonbarron_website">template</a> which is created by <a target="_blank" href="http://jonbarron.info">Jon Barron</a>.
              </p>
              <p style="text-align:center;font-size:small;">
                
                <a target="_blank" href="https://clustrmaps.com/site/1bppj" title="Visit tracker"><img src="https://www.clustrmaps.com/map_v2.png?cl=808080&w=a&t=m&d=pWP0RVNwu3SOBb8nk_6bAcI8i3zvJYKNXLNmGHEzU6o&co=ffffff&ct=808080"></a>
                
                <!-- 
                <a href="https://clustrmaps.com/site/1b0l1" title="Visit tracker"> </a>
                -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
